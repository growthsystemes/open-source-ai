# Configuration Docker pour inference-optim-llm

# Modèle à utiliser
MODEL_ID=gpt2

# Paramètres de benchmark
BATCH_SIZE=1
MAX_NEW_TOKENS=32

# Configuration GPU (laissez vide pour CPU seulement)
CUDA_VISIBLE_DEVICES=0

# Configuration TensorRT-LLM
TRT_PRECISION=fp16

# Cache HuggingFace (optionnel, utilisera le volume Docker par défaut)
# HF_HOME=/workspace/.cache/huggingface