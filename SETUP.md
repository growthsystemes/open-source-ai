# üöÄ Guide de Setup - Inference Optim LLM

Ce guide vous accompagne √©tape par √©tape pour configurer et utiliser le projet.

## ü™ü **UTILISATEURS WINDOWS** ‚Üí [README_WINDOWS.md](README_WINDOWS.md)

**Probl√®me PyTorch ?** ‚Üí [CORRECTION_URGENTE.md](CORRECTION_URGENTE.md)

## üìã Pr√©requis

- **Python 3.9+** (test√© avec 3.9, 3.10, 3.11)
- **GPU NVIDIA** avec CUDA 11.8+ (optionnel pour baseline)
- **8GB+ RAM** (16GB+ recommand√©)
- **20GB+ espace disque** (pour les mod√®les)

## ‚ö° Installation Rapide

### 1. Clone et Navigation
```bash
cd inference-optim-LLM
```

### 2. Installation Automatique selon OS
```bash
# D√©tection automatique Windows/Linux/Mac
python quick_start.py

# Alternative: installation manuelle
python setup_dev.py
```

### 3. Validation
```bash
# Test rapide
python quick_test_windows.py  # Windows
python scripts/validate_setup.py  # Tous OS
```

## üîß Installation Manuelle

### 1. Environnement Python
```bash
# Avec conda (recommand√©)
conda create -n optim-llm python=3.10
conda activate optim-llm

# Ou avec venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### 2. Installation des D√©pendances
```bash
# Installation compl√®te
pip install -r requirements-dev.txt

# Ou installation minimale
pip install -e .
pip install torch transformers typer rich huggingface-hub
```

### 3. Configuration
```bash
# Copiez le fichier d'exemple
cp .env.example .env

# √âditez selon vos besoins
nano .env  # ou votre √©diteur pr√©f√©r√©
```

## üß™ Tests de Validation

### Test CLI
```bash
# Aide g√©n√©rale
iol --help
python -m inference_optim_llm.cli --help

# Aide commandes sp√©cifiques
iol run --help
iol bench --help
```

### Test Baseline Simple
```bash
# Test avec mod√®le l√©ger (pour validation)
MODEL_ID=gpt2 iol run baseline --max-new-tokens 10
```

### Test Complet (avec GPU)
```bash
# Benchmark complet
iol bench --batch-size 1 --max-new-tokens 64
```

## üê≥ Utilisation Docker

### Setup Docker
```bash
cd docker

# Build des images
docker-compose build

# Test baseline
docker-compose --profile baseline up

# Benchmark complet
docker-compose --profile bench up
```

### D√©veloppement Docker
```bash
# Mode d√©veloppement interactif
docker-compose --profile dev up
docker-compose exec dev bash
```

## üìä Premiers Pas

### 1. Test Baseline
```bash
# Test rapide avec un petit mod√®le
iol run baseline --prompts-file data/prompts.txt --max-new-tokens 32

# Sauvegarde des r√©sultats
iol run baseline --save-json reports/test.jsonl
```

### 2. Benchmark Complet
```bash
# Benchmark baseline vs TensorRT-LLM
iol bench --prompts-file data/prompts.txt --output-dir reports/

# G√©n√©ration des graphiques
python scripts/export_results.py --reports-dir reports/
```

### 3. Analyse des R√©sultats
```bash
# Analyse automatique
python scripts/benchmark.py --reports-dir reports/

# V√©rification des fichiers g√©n√©r√©s
ls reports/
# ‚Üí baseline.jsonl, trtllm.jsonl, benchmark_results.md
```

## üîß Configuration Avanc√©e

### Variables d'Environnement (.env)
```bash
# Mod√®le √† utiliser
MODEL_ID=meta-llama/Llama-2-7b-chat-hf

# Configuration GPU
CUDA_VISIBLE_DEVICES=0
TRT_PRECISION=fp16

# Performance
BATCH_SIZE=1
MAX_NEW_TOKENS=64

# Cache
HF_HOME=/path/to/cache
```

### Configuration Multi-GPU
```bash
# Dans .env
CUDA_VISIBLE_DEVICES=0,1
DEVICE_MAP=balanced_low_0

# Ou en ligne de commande
iol run baseline --device-map balanced_low_0
```

## üö® R√©solution de Probl√®mes

### Erreur d'Import
```bash
# R√©installez le package
pip install -e .

# V√©rifiez PYTHONPATH
export PYTHONPATH=$PWD:$PYTHONPATH
```

### Probl√®me GPU/CUDA
```bash
# V√©rifiez CUDA
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Test CPU uniquement
CUDA_VISIBLE_DEVICES="" iol run baseline
```

### Mod√®le Introuvable
```bash
# T√©l√©chargement manuel
iol download meta-llama/Llama-2-7b-chat-hf

# Ou mod√®le plus petit pour test
MODEL_ID=gpt2 iol run baseline
```

### M√©moire Insuffisante
```bash
# Utilisez un mod√®le plus petit
MODEL_ID=gpt2 iol run baseline

# Ou r√©duisez le batch size
iol run baseline --batch-size 1 --max-new-tokens 32
```

## üìà Monitoring et Profiling

### Monitoring GPU
```bash
# En parall√®le pendant l'ex√©cution
watch -n 1 nvidia-smi

# Logs avec m√©triques d√©taill√©es
iol run baseline --save-json metrics.jsonl
```

### Profiling Avanc√©
```bash
# Avec profiling Python
python -m cProfile -o profile.stats scripts/run_baseline.py

# Analyse du profil
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)"
```

## üéØ Workflow de D√©veloppement

### 1. D√©veloppement Local
```bash
# Tests unitaires
pytest tests/ -v

# Linting
ruff check . && ruff format .

# Type checking
mypy inference_optim_llm/
```

### 2. Tests d'Int√©gration
```bash
# Validation compl√®te
python scripts/validate_setup.py

# Test pipeline complet
iol bench --skip-trtllm  # Baseline seulement
```

### 3. CI/CD
```bash
# Simulation CI locale
pytest tests/ --cov=inference_optim_llm
ruff check .
mypy inference_optim_llm/
```

## üìö Ressources Suppl√©mentaires

- **Documentation API** : Voir docstrings dans le code
- **Exemples** : R√©pertoire `notebooks/`
- **Rapports** : G√©n√©r√©s dans `reports/`
- **Logs** : Configuration dans `pyproject.toml`

## üÜò Support

En cas de probl√®me :

1. V√©rifiez cette documentation
2. Ex√©cutez `python scripts/validate_setup.py`
3. Consultez les logs d√©taill√©s
4. Cr√©ez une issue avec les d√©tails de l'erreur