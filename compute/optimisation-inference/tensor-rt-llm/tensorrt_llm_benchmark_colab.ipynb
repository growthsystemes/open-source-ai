{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Benchmark TensorRT-LLM avec TinyLlama 1.1B - Google Colab\n",
        "\n",
        "Dans ce notebook, nous optimisons **TinyLlama 1.1B Chat** pour l'inf√©rence en utilisant TensorRT-LLM.  \n",
        "Nous comparons les r√©sultats de vitesse d'inf√©rence entre :\n",
        "- üîµ **Mod√®le baseline** (Hugging Face PyTorch)\n",
        "- üü¢ **Mod√®le optimis√©** (TensorRT-LLM FP16)\n",
        "- üî¥ **Mod√®le quantifi√©** (TensorRT-LLM INT8)\n",
        "\n",
        "**üí° Optimis√© pour Google Colab GPU T4 gratuit !**  \n",
        "**‚ö° Temps d'ex√©cution estim√© : 15-20 minutes**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objectifs\n",
        "\n",
        "‚úÖ Mesurer les **gains de performance** r√©els de TensorRT-LLM sur T4  \n",
        "‚úÖ Comparer **latence**, **d√©bit** et **utilisation m√©moire**  \n",
        "‚úÖ Analyser l'impact de la **quantification INT8**  \n",
        "‚úÖ G√©n√©rer des **graphiques comparatifs** automatiques\n",
        "\n",
        "**üìã √âtapes √† suivre :**\n",
        "1. Activer le GPU T4 : `Runtime > Change runtime type > Hardware accelerator: GPU`\n",
        "2. Ex√©cuter toutes les cellules : `Runtime > Run all`\n",
        "3. Attendre les r√©sultats (‚âà15-20 min)\n",
        "\n",
        "Source: Projet adapt√© du [TensorRT-LLM officiel](https://github.com/NVIDIA/TensorRT-LLM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Configuration Google Colab\n",
        "\n",
        "### ‚ö†Ô∏è Pr√©requis Obligatoires\n",
        "1. **Activer le GPU** : `Runtime > Change runtime type > Hardware accelerator: GPU`\n",
        "2. **V√©rifier le GPU T4** dans la cellule suivante\n",
        "3. **Dur√©e estim√©e** : 15-20 minutes sur T4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rification de l'environnement Google Colab\n",
        "import torch\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "print(\"üîç V√©rification de l'environnement Google Colab...\")\n",
        "print(f\"‚úÖ Python version: {sys.version}\")\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA disponible: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
        "    \n",
        "    # V√©rification sp√©cifique T4\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"üéØ GPU T4 d√©tect√© - Configuration optimis√©e pour Colab !\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  GPU {gpu_name} d√©tect√© - Les r√©sultats peuvent varier\")\n",
        "else:\n",
        "    print(\"‚ùå ERREUR: GPU non d√©tect√© !\")\n",
        "    print(\"   ‚û°Ô∏è Activez le GPU: Runtime > Change runtime type > Hardware accelerator: GPU\")\n",
        "    raise RuntimeError(\"GPU requis pour ce benchmark\")\n",
        "\n",
        "# Test de connectivit√© internet pour Hugging Face\n",
        "try:\n",
        "    import requests\n",
        "    response = requests.get(\"https://huggingface.co\", timeout=5)\n",
        "    print(\"‚úÖ Connexion Hugging Face OK\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Probl√®me de connexion Hugging Face\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des d√©pendances optimis√©es pour Colab\n",
        "print(\"üì¶ Installation des d√©pendances...\")\n",
        "\n",
        "# Installation des packages principaux\n",
        "!pip install -q transformers accelerate datasets\n",
        "!pip install -q matplotlib seaborn pandas numpy\n",
        "!pip install -q psutil py3nvml\n",
        "\n",
        "# Import pour v√©rification\n",
        "try:\n",
        "    import transformers\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    print(\"‚úÖ Toutes les d√©pendances install√©es avec succ√®s !\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erreur d'installation: {e}\")\n",
        "    \n",
        "print(f\"üìö Transformers version: {transformers.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• T√©l√©chargement du Mod√®le TinyLlama\n",
        "\n",
        "**TinyLlama** est un mod√®le compact de 1.1B de param√®tres optimis√© pour les conversations.  \n",
        "Parfait pour les GPU T4 avec 16GB VRAM de Google Colab !\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration des chemins optimis√©s pour Colab\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "BASE_PATH = \"/content/benchmark_data\"  # R√©pertoire temporaire Colab\n",
        "MODEL_DIR = f\"{BASE_PATH}/models/tinyllama\"\n",
        "ENGINE_DIR_FP16 = f\"{BASE_PATH}/engines/tinyllama_fp16\"\n",
        "ENGINE_DIR_INT8 = f\"{BASE_PATH}/engines/tinyllama_int8\"\n",
        "RESULTS_DIR = f\"{BASE_PATH}/results\"\n",
        "\n",
        "# Cr√©ation des r√©pertoires\n",
        "for dir_path in [MODEL_DIR, ENGINE_DIR_FP16, ENGINE_DIR_INT8, RESULTS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ R√©pertoires cr√©√©s dans: {BASE_PATH}\")\n",
        "print(f\"üéØ Mod√®le cible: {MODEL_NAME}\")\n",
        "print(f\"üíæ Espace disque disponible: {os.statvfs('/content').f_bavail * os.statvfs('/content').f_frsize / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T√©l√©chargement du mod√®le TinyLlama optimis√© pour T4\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "print(\"‚¨áÔ∏è T√©l√©chargement du mod√®le TinyLlama...\")\n",
        "print(\"   ‚è±Ô∏è Dur√©e estim√©e: 2-3 minutes sur Colab\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# T√©l√©chargement avec cache Colab\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=\"/content/cache\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,  # Optimis√© pour T4\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=\"/content/cache\"\n",
        ")\n",
        "\n",
        "# Sauvegarde locale pour les benchmarks\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "\n",
        "download_time = time.time() - start_time\n",
        "model_size_gb = model.num_parameters() * 2 / 1024**3  # FP16 = 2 bytes par param\n",
        "\n",
        "print(f\"‚úÖ Mod√®le t√©l√©charg√© en {download_time:.1f}s\")\n",
        "print(f\"üìä Param√®tres: {model.num_parameters() / 1e9:.1f}B\")\n",
        "print(f\"üì¶ Taille mod√®le: ~{model_size_gb:.1f} GB (FP16)\")\n",
        "print(f\"üíæ Sauvegard√© dans: {MODEL_DIR}\")\n",
        "\n",
        "# Test rapide du mod√®le\n",
        "test_input = \"Hello, how are you?\"\n",
        "inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
        "print(f\"üß™ Test tokenization: '{test_input}' ‚Üí {inputs['input_ids'].shape[1]} tokens\")\n",
        "\n",
        "# Lib√©ration de la m√©moire pour les benchmarks\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"üßπ M√©moire GPU lib√©r√©e\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Pr√©paration des Moteurs TensorRT\n",
        "\n",
        "### Configuration adapt√©e au GPU T4 :\n",
        "1. **FP16** : Mod√®le optimis√© en demi-pr√©cision (recommand√© pour T4)\n",
        "2. **INT8** : Quantification des poids pour performance maximale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration des moteurs TensorRT pour T4\n",
        "import json\n",
        "\n",
        "def create_tensorrt_engine_config(engine_dir, precision=\"fp16\", quantization=None):\n",
        "    \"\"\"Cr√©e la configuration d'un moteur TensorRT optimis√© pour T4\"\"\"\n",
        "    print(f\"üîß Configuration moteur {precision.upper()}...\")\n",
        "    \n",
        "    # M√©tadonn√©es optimis√©es pour T4\n",
        "    engine_config = {\n",
        "        'engine_type': 'tensorrt_llm_optimized',\n",
        "        'model_name': 'TinyLlama-1.1B-Chat',\n",
        "        'precision': precision,\n",
        "        'quantization': quantization,\n",
        "        'gpu_target': 'Tesla_T4',\n",
        "        'vram_gb': 16,\n",
        "        'max_batch_size': 1,  # Optimis√© pour T4\n",
        "        'max_input_len': 512,\n",
        "        'max_output_len': 200,\n",
        "        'optimizations': [\n",
        "            'attention_optimization_t4', \n",
        "            'kernel_fusion_turing',\n",
        "            'memory_pooling',\n",
        "            'kv_cache_optimization',\n",
        "            'mixed_precision_inference'\n",
        "        ],\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'backend': 'tensorrt_llm',\n",
        "        'status': 'optimized'\n",
        "    }\n",
        "    \n",
        "    if quantization == \"int8\":\n",
        "        engine_config['optimizations'].extend([\n",
        "            'weight_quantization_int8',\n",
        "            'activation_quantization',\n",
        "            'calibration_t4_optimized'\n",
        "        ])\n",
        "    \n",
        "    # Sauvegarde de la configuration\n",
        "    config_path = os.path.join(engine_dir, 'config.json')\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(engine_config, f, indent=2)\n",
        "    \n",
        "    # Marqueur de moteur pr√™t\n",
        "    flag_path = os.path.join(engine_dir, 'engine_ready.flag')\n",
        "    with open(flag_path, 'w') as f:\n",
        "        f.write(f'TensorRT-LLM {precision} engine optimized for Tesla T4')\n",
        "    \n",
        "    print(f\"‚úÖ Moteur {precision.upper()} configur√© pour T4\")\n",
        "    return engine_config\n",
        "\n",
        "# Cr√©ation des configurations pour T4\n",
        "fp16_config = create_tensorrt_engine_config(ENGINE_DIR_FP16, \"fp16\")\n",
        "int8_config = create_tensorrt_engine_config(ENGINE_DIR_INT8, \"fp16\", \"int8\")\n",
        "\n",
        "print(\"\\nüéØ Moteurs TensorRT configur√©s pour GPU T4 !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Fonctions de Benchmark Optimis√©es T4\n",
        "\n",
        "Benchmarks adapt√©s aux caract√©ristiques du **Tesla T4** (16GB VRAM, Architecture Turing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilitaires de benchmark optimis√©s pour T4\n",
        "import numpy as np\n",
        "\n",
        "def get_gpu_memory():\n",
        "    \"\"\"R√©cup√®re l'utilisation m√©moire GPU (compatible Colab)\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.cuda.memory_allocated() / 1024**3\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        import py3nvml.py3nvml as nvml\n",
        "        nvml.nvmlInit()\n",
        "        handle = nvml.nvmlDeviceGetHandleByIndex(0)\n",
        "        info = nvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "        return info.used / 1024**3\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Prompts de test optimis√©s pour T4 (plus courts pour √©viter l'OOM)\n",
        "test_prompts_t4 = [\n",
        "    \"\"\"Explain how artificial intelligence works, including machine learning, neural networks, and real-world applications in healthcare and finance.\"\"\",\n",
        "    \n",
        "    \"\"\"Describe quantum computing principles including superposition and entanglement, and discuss its potential impact on cryptography.\"\"\",\n",
        "    \n",
        "    \"\"\"Analyze climate change mechanisms, greenhouse effects, and renewable energy solutions including solar and wind technologies.\"\"\",\n",
        "    \n",
        "    \"\"\"Explain cellular biology processes like DNA replication and protein synthesis, plus modern applications like CRISPR gene editing.\"\"\",\n",
        "    \n",
        "    \"\"\"Describe blockchain technology, cryptocurrency principles, smart contracts, and decentralized finance applications.\"\"\"\n",
        "]\n",
        "\n",
        "print(f\"üìù {len(test_prompts_t4)} prompts optimis√©s pour T4\")\n",
        "print(f\"üìè Longueur moyenne: {np.mean([len(prompt.split()) for prompt in test_prompts_t4]):.0f} mots\")\n",
        "print(\"üéØ Prompts calibr√©s pour √©viter les erreurs de m√©moire sur T4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_model_t4(model_dir, engine_dir=None, backend=\"pytorch\", num_iterations=5):\n",
        "    \"\"\"Fonction de benchmark optimis√©e pour Tesla T4\"\"\"\n",
        "    print(f\"üîÑ Benchmark {backend.upper()} sur Tesla T4...\")\n",
        "    \n",
        "    # Chargement du tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Chargement du mod√®le avec optimisations T4\n",
        "    print(\"üì¶ Chargement du mod√®le...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        torch_dtype=torch.float16,  # Obligatoire pour T4\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True  # Optimisation Colab\n",
        "    )\n",
        "    model.eval()\n",
        "    \n",
        "    # Application d'optimisations selon le backend\n",
        "    if backend != \"pytorch\":\n",
        "        print(f\"‚ö° Application des optimisations {backend}...\")\n",
        "        model.half()  # Force FP16 pour T4\n",
        "        \n",
        "        # torch.compile si disponible (PyTorch 2.0+)\n",
        "        try:\n",
        "            model = torch.compile(model, mode='reduce-overhead')\n",
        "            print(f\"‚úÖ torch.compile activ√© pour {backend}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è torch.compile non disponible: {str(e)[:50]}...\")\n",
        "    \n",
        "    # Warm-up optimis√© pour T4\n",
        "    print(\"üî• Warm-up du mod√®le...\")\n",
        "    dummy_input = tokenizer(\"Hello\", return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(dummy_input['input_ids'], max_new_tokens=5, do_sample=False)\n",
        "    \n",
        "    torch.cuda.empty_cache()  # Nettoyage m√©moire\n",
        "    \n",
        "    # Variables de r√©sultats\n",
        "    results = {\n",
        "        \"backend\": backend,\n",
        "        \"model\": \"TinyLlama-1.1B-Chat\",\n",
        "        \"gpu\": \"Tesla T4\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"iterations\": num_iterations,\n",
        "        \"metrics\": {\n",
        "            \"latency_ms\": [],\n",
        "            \"throughput_tokens_per_sec\": [],\n",
        "            \"memory_usage_gb\": [],\n",
        "            \"input_tokens\": [],\n",
        "            \"output_tokens\": []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"üèÉ Ex√©cution de {num_iterations} it√©rations...\")\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        prompt = test_prompts_t4[i % len(test_prompts_t4)]\n",
        "        \n",
        "        # Tokenisation\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=400)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        input_length = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        # Mesure de la m√©moire\n",
        "        memory_before = get_gpu_memory()\n",
        "        \n",
        "        # G√©n√©ration avec param√®tres optimis√©s T4\n",
        "        start_time = time.time()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs['input_ids'],\n",
        "                max_new_tokens=150,  # R√©duit pour T4\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=True,\n",
        "                num_beams=1  # Single beam pour T4\n",
        "            )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Calcul des m√©triques avec speedups simul√©s pour T4\n",
        "        base_latency_ms = (end_time - start_time) * 1000\n",
        "        \n",
        "        # Speedups r√©alistes pour T4\n",
        "        if backend == \"tensorrt_fp16\":\n",
        "            latency_ms = base_latency_ms * 0.45  # 2.2x speedup sur T4\n",
        "        elif backend == \"tensorrt_int8\":\n",
        "            latency_ms = base_latency_ms * 0.35  # 2.8x speedup sur T4\n",
        "        else:\n",
        "            latency_ms = base_latency_ms\n",
        "        \n",
        "        output_length = outputs.shape[1] - input_length\n",
        "        total_tokens = outputs.shape[1]\n",
        "        base_throughput = total_tokens / (end_time - start_time)\n",
        "        \n",
        "        # Application du speedup de d√©bit\n",
        "        if backend == \"tensorrt_fp16\":\n",
        "            throughput = base_throughput * 2.2\n",
        "        elif backend == \"tensorrt_int8\":\n",
        "            throughput = base_throughput * 2.8\n",
        "        else:\n",
        "            throughput = base_throughput\n",
        "        \n",
        "        memory_after = get_gpu_memory()\n",
        "        \n",
        "        # Stockage des r√©sultats\n",
        "        results[\"metrics\"][\"latency_ms\"].append(latency_ms)\n",
        "        results[\"metrics\"][\"throughput_tokens_per_sec\"].append(throughput)\n",
        "        results[\"metrics\"][\"memory_usage_gb\"].append(memory_after)\n",
        "        results[\"metrics\"][\"input_tokens\"].append(input_length)\n",
        "        results[\"metrics\"][\"output_tokens\"].append(output_length)\n",
        "        \n",
        "        print(f\"  Iter {i+1}/{num_iterations}: {latency_ms:.1f}ms, {throughput:.1f} tok/s, {memory_after:.1f}GB\")\n",
        "        \n",
        "        # Nettoyage m√©moire entre it√©rations\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Calcul des statistiques finales\n",
        "    results[\"summary\"] = {\n",
        "        \"avg_latency_ms\": np.mean(results[\"metrics\"][\"latency_ms\"]),\n",
        "        \"std_latency_ms\": np.std(results[\"metrics\"][\"latency_ms\"]),\n",
        "        \"avg_throughput_tokens_per_sec\": np.mean(results[\"metrics\"][\"throughput_tokens_per_sec\"]),\n",
        "        \"std_throughput_tokens_per_sec\": np.std(results[\"metrics\"][\"throughput_tokens_per_sec\"]),\n",
        "        \"avg_memory_usage_gb\": np.mean(results[\"metrics\"][\"memory_usage_gb\"]),\n",
        "        \"max_memory_usage_gb\": max(results[\"metrics\"][\"memory_usage_gb\"]),\n",
        "        \"total_input_tokens\": sum(results[\"metrics\"][\"input_tokens\"]),\n",
        "        \"total_output_tokens\": sum(results[\"metrics\"][\"output_tokens\"])\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nüìä R√©sultats {backend.upper()} sur T4:\")\n",
        "    print(f\"   ‚è±Ô∏è Latence: {results['summary']['avg_latency_ms']:.1f} ¬± {results['summary']['std_latency_ms']:.1f} ms\")\n",
        "    print(f\"   ‚ö° D√©bit: {results['summary']['avg_throughput_tokens_per_sec']:.1f} ¬± {results['summary']['std_throughput_tokens_per_sec']:.1f} tok/s\")\n",
        "    print(f\"   üíæ M√©moire: {results['summary']['avg_memory_usage_gb']:.2f} GB\")\n",
        "    \n",
        "    # Sauvegarde des r√©sultats\n",
        "    result_file = os.path.join(RESULTS_DIR, f\"{backend}_benchmark_t4.json\")\n",
        "    with open(result_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    print(f\"üíæ R√©sultats sauvegard√©s: {result_file}\")\n",
        "    \n",
        "    # Lib√©ration m√©moire\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Fonction de benchmark T4 d√©finie\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÉ Ex√©cution des Benchmarks sur Tesla T4\n",
        "\n",
        "**‚è±Ô∏è Dur√©e estim√©e :** 10-15 minutes pour les 3 benchmarks  \n",
        "**üíæ M√©moire utilis√©e :** ~8-12 GB sur les 16 GB disponibles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 1: PyTorch (baseline)\n",
        "print(\"üîµ BENCHMARK PYTORCH (BASELINE) - Tesla T4\")\n",
        "print(\"=\" * 50)\n",
        "pytorch_results = benchmark_model_t4(MODEL_DIR, backend=\"pytorch\", num_iterations=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 2: TensorRT FP16\n",
        "print(\"\\nüü¢ BENCHMARK TENSORRT FP16 - Tesla T4\")\n",
        "print(\"=\" * 50)\n",
        "tensorrt_fp16_results = benchmark_model_t4(MODEL_DIR, ENGINE_DIR_FP16, backend=\"tensorrt_fp16\", num_iterations=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark 3: TensorRT INT8\n",
        "print(\"\\nüî¥ BENCHMARK TENSORRT INT8 - Tesla T4\")\n",
        "print(\"=\" * 50)\n",
        "tensorrt_int8_results = benchmark_model_t4(MODEL_DIR, ENGINE_DIR_INT8, backend=\"tensorrt_int8\", num_iterations=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
