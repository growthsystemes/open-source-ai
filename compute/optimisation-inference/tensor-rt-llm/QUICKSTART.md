# üöÄ Guide de D√©marrage Rapide - TensorRT-LLM Benchmark

Benchmarker TensorRT-LLM vs PyTorch en 5 minutes !

## ‚ö° D√©marrage Ultra-Rapide

```bash
# 1. Cloner le projet
git clone <votre-repo>
cd tensor-rt-llm

# 2. üîê OBLIGATOIRE : S'authentifier sur NVIDIA NGC
.\setup_nvidia_auth.ps1     # Windows
./setup_nvidia_auth.sh      # Linux/macOS

# 3. Lancer le benchmark complet (20-30 min)
docker-compose --profile auto up benchmark-full

# 4. Voir les r√©sultats
ls data/results/
```

## üõ†Ô∏è Options de Lancement

### Windows (PowerShell)
```powershell
# Benchmark complet
.\run_benchmark.ps1 -Mode all

# √âtapes individuelles
.\run_benchmark.ps1 -Mode build      # Construction moteur
.\run_benchmark.ps1 -Mode pytorch    # Benchmark PyTorch
.\run_benchmark.ps1 -Mode tensorrt   # Benchmark TensorRT
.\run_benchmark.ps1 -Mode compare    # Comparaison

# Mode interactif
.\run_benchmark.ps1 -Mode interactive
```

### Linux/macOS (Make)
```bash
# Benchmark complet
make all

# Ou automatique (tout en une fois)
make auto

# √âtapes individuelles
make build      # Construction moteur
make pytorch    # Benchmark PyTorch  
make tensorrt   # Benchmark TensorRT
make compare    # Comparaison

# Mode interactif
make interactive

# Aide
make help
```

### Docker Compose Direct
```bash
# Benchmark complet automatique
docker-compose --profile auto up benchmark-full

# √âtapes manuelles
docker-compose --profile build-only up build-engine
docker-compose --profile pytorch-only up benchmark-pytorch
docker-compose --profile tensorrt-only up benchmark-tensorrt
docker-compose --profile compare-only up compare-results

# Mode interactif
docker-compose --profile manual run --rm tensorrt-llm-benchmark
```

## üîç Validation de l'Environnement

Avant de commencer, validez votre setup :

```bash
# Validation automatique
python scripts/validate_setup.py

# Test GPU Docker
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu20.04 nvidia-smi
```

## üìä R√©sultats

Apr√®s ex√©cution, consultez `data/results/` :

- **`pytorch_benchmark.json`** - M√©triques PyTorch d√©taill√©es
- **`tensorrt_benchmark.json`** - M√©triques TensorRT d√©taill√©es
- **`benchmark_comparison_report.json`** - Rapport complet
- **`benchmark_comparison.png`** - Graphiques de comparaison

### Exemple de R√©sultats

```json
{
  "speedup_analysis": {
    "latency_improvement": {
      "pytorch_ms": 1250.5,
      "tensorrt_ms": 345.2,
      "speedup_factor": 3.62,
      "improvement_percent": 72.4
    },
    "throughput_improvement": {
      "pytorch_tokens_per_sec": 18.7,
      "tensorrt_tokens_per_sec": 67.8,
      "speedup_factor": 3.63,
      "improvement_percent": 262.6
    }
  }
}
```

## üéÆ Configuration GPU

### NVIDIA GPUs Test√©s
- ‚úÖ **RTX 4090** - Excellent (12+ GB VRAM)
- ‚úÖ **RTX 4080** - Tr√®s bon (10+ GB VRAM)  
- ‚úÖ **RTX 3080** - Bon (8+ GB VRAM)
- ‚ö†Ô∏è **RTX 3060** - Limit√© (6 GB VRAM)
- ‚ùå **GTX 1660** - Incompatible (pas de Tensor Cores)

### Pr√©requis
- **VRAM**: 6+ GB (8+ GB recommand√©)
- **Drivers**: 525+ 
- **CUDA**: 12.0+ (inclus dans l'image Docker)
- **Docker**: Avec support GPU

## üîß D√©pannage Express

### Erreur "CUDA out of memory"
```bash
# R√©duire la taille de batch/s√©quence
# Modifier scripts/build_engine.sh:
--max_batch_size 1
--max_input_len 256
--max_output_len 256
```

### Erreur "Docker can't access GPU"
```bash
# Windows (Docker Desktop)
# Activer "Use WSL 2 based engine" + "GPU support"

# Linux
sudo apt install nvidia-docker2
sudo systemctl restart docker
```

### Erreur "TensorRT engine not found"
```bash
# Reconstruire le moteur
docker-compose --profile build-only up build-engine
```

## üìà R√©sultats Attendus

### TinyLlama 1.1B sur RTX 4090
| M√©trique | PyTorch | TensorRT | Gain |
|----------|---------|----------|------|
| Latence | ~1200ms | ~350ms | **3.4x** |
| D√©bit | ~20 tok/s | ~70 tok/s | **3.5x** |
| M√©moire | ~2.8GB | ~2.0GB | **-29%** |

### TinyLlama 1.1B sur RTX 3080
| M√©trique | PyTorch | TensorRT | Gain |
|----------|---------|----------|------|
| Latence | ~1800ms | ~550ms | **3.3x** |
| D√©bit | ~15 tok/s | ~50 tok/s | **3.3x** |
| M√©moire | ~3.1GB | ~2.2GB | **-29%** |

## üöÄ Prochaines √âtapes

1. **Tester d'autres mod√®les** :
   ```bash
   # Modifier MODEL_NAME dans scripts/build_engine.sh
   MODEL_NAME="microsoft/DialoGPT-medium"
   ```

2. **Optimisations avanc√©es** :
   ```bash
   # Ajouter dans build_engine.sh
   --use_fused_mlp
   --use_gpt_attention_plugin
   --int8  # Quantization
   ```

3. **Batch processing** :
   ```bash
   # Augmenter le batch size
   --max_batch_size 4
   ```

## üìö Documentation Compl√®te

- Voir [README.md](README.md) pour la documentation compl√®te
- Configuration avanc√©e dans [config.example.env](config.example.env)
- Scripts d√©taill√©s dans [scripts/](scripts/)

---

**üéØ TL;DR** : `docker-compose --profile auto up benchmark-full` et attendez 20-30 minutes ! üöÄ
