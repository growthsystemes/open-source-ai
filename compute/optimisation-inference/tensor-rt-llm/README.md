#  Benchmark TensorRT-LLM sur TinyLlama avec Docker

Mesure le gain de performances qu'apporte [NVIDIA TensorRT‚ÄëLLM](https://github.com/NVIDIA/TensorRT-LLM) avec un petit mod√®le open‚Äësource Hugging Face (TinyLlama 1.1B Chat) par rapport √† l'inf√©rence PyTorch "pure". Tout le projet tourne dans un conteneur Docker pr√™t √† l'emploi : aucune compilation locale n'est n√©cessaire.

## Objectifs

- Reproduire facilement un flux complet : t√©l√©charger le mod√®le ‚Üí convertir/optimiser ‚Üí g√©n√©rer un moteur TensorRT ‚Üí benchmarker
- Obtenir des m√©triques latency & throughput ¬´ out‚Äëof‚Äëthe‚Äëbox ¬ª
- Servir de point de d√©part pour tester d'autres LLM, quantisations ou GPUs

## üìÅ Arborescence

```
.
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile              # Image bas√©e sur nvcr.io/nvidia/tensorrt‚Äëllm
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ build_engine.sh         # Construction du moteur TensorRT
‚îÇ   ‚îú‚îÄ‚îÄ bench_pytorch.sh        # Benchmark r√©f√©rence (PyTorch)
‚îÇ   ‚îú‚îÄ‚îÄ bench_trt.sh           # Benchmark optimis√© (TensorRT-LLM)
‚îÇ   ‚îî‚îÄ‚îÄ compare.py             # Analyse et comparaison des r√©sultats
‚îú‚îÄ‚îÄ docker-compose.yml         # Services Docker pour faciliter l'utilisation
‚îú‚îÄ‚îÄ data/                      # Donn√©es persistantes (cr√©√© automatiquement)
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Mod√®les t√©l√©charg√©s
‚îÇ   ‚îú‚îÄ‚îÄ engines/              # Moteurs TensorRT compil√©s
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/          # Checkpoints interm√©diaires
‚îÇ   ‚îî‚îÄ‚îÄ results/              # R√©sultats de benchmark
‚îî‚îÄ‚îÄ README.md
```

## üõ†Pr√©requis

- **Docker** avec support GPU (`nvidia-docker` ou Docker Desktop avec GPU)
- **GPU NVIDIA** compatible CUDA (RTX 20xx/30xx/40xx recommand√©)
- **8+ GB de VRAM** (minimum pour TinyLlama 1.1B)
- **Drivers NVIDIA** r√©cents (525+)
- **Compte NVIDIA NGC** (gratuit) pour acc√©der aux images TensorRT-LLM

### √âTAPE OBLIGATOIRE : Authentification NVIDIA

**Avant de commencer**, vous devez vous authentifier sur le registry NVIDIA :

#### Option 1 : Script Automatique (Recommand√©)

**Windows :**
```powershell
.\setup_nvidia_auth.ps1
```

**Linux/macOS :**
```bash
chmod +x setup_nvidia_auth.sh
./setup_nvidia_auth.sh
```

#### Option 2 : Manuel

1. **Cr√©er un compte NGC** sur [ngc.nvidia.com](https://ngc.nvidia.com) (gratuit)
2. **G√©n√©rer une cl√© API** : Profile ‚Üí Setup ‚Üí Generate API Key
3. **Se connecter** :
   ```bash
   docker login nvcr.io
   # Username: $oauthtoken
   # Password: [votre_cl√©_API_qui_commence_par_nvapi-]
   ```

**Guide d√©taill√©** : Voir [NVIDIA_SETUP.md](NVIDIA_SETUP.md)

### V√©rification GPU

```bash
# V√©rifier que Docker peut acc√©der au GPU
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu20.04 nvidia-smi
```

## Utilisation Rapide

### Commande Recommand√©e (Une Ligne)

Apr√®s avoir configur√© l'authentification NVIDIA, lancez le benchmark complet :

```bash
docker-compose --profile auto up benchmark-full
```

**‚è±Dur√©e estim√©e** : 15-25 minutes selon votre GPU  
**R√©sultats** : G√©n√©r√©s automatiquement dans `./data/results/`

### Pipeline Complet Ex√©cut√©

Cette commande ex√©cute automatiquement :
1. **Construction du moteur TensorRT** (`build_engine.sh`)
2. **Benchmark PyTorch** (r√©f√©rence baseline)
3. **Benchmark TensorRT-LLM** (version optimis√©e)
4. **Analyse comparative** (graphiques + rapport JSON)

### √âtapes Manuelles (Contr√¥le Avanc√©)

Pour plus de contr√¥le sur chaque √©tape :

```bash
# 1. Construction du moteur TensorRT-LLM
docker-compose --profile build-only up build-engine

# 2. Benchmark PyTorch (r√©f√©rence baseline)  
docker-compose --profile pytorch-only up benchmark-pytorch

# 3. Benchmark TensorRT-LLM (version optimis√©e)
docker-compose --profile tensorrt-only up benchmark-tensorrt

# 4. Comparaison et analyse des r√©sultats
docker-compose --profile compare-only up compare-results
```

### Mode Interactif (Debug/Exploration)

Pour explorer le syst√®me ou d√©boguer :

```bash
# D√©marrage du container en mode interactif
docker-compose --profile manual run --rm tensorrt-llm-benchmark bash

# Dans le container, vous pouvez ex√©cuter :
bash scripts/build_engine.sh
bash scripts/bench_pytorch.sh  
bash scripts/bench_trt.sh
python3 scripts/compare.py

# Ou explorer les mod√®les/engines :
ls -la data/models/
ls -la data/engines/
cat data/results/pytorch_benchmark.json
```

## R√©sultats

Apr√®s ex√©cution, vous trouverez dans `./data/results/` :

### Fichiers G√©n√©r√©s

- `pytorch_benchmark.json` - M√©triques d√©taill√©es PyTorch
- `tensorrt_benchmark.json` - M√©triques d√©taill√©es TensorRT-LLM  
- `benchmark_comparison_report.json` - Rapport de comparaison complet
- `benchmark_comparison.png` - Graphiques de comparaison

### M√©triques Analys√©es

- **‚è±Latence** : Temps de g√©n√©ration par s√©quence (ms)
- **D√©bit** : Tokens g√©n√©r√©s par seconde
- **M√©moire GPU** : Utilisation VRAM (GB)
- **Speedup** : Facteur d'am√©lioration TensorRT vs PyTorch

### Exemple de Sortie R√©elle (RTX 4070)

```
RAPPORT DE PERFORMANCE - TensorRT-LLM vs PyTorch
============================================================

LATENCE:
   PyTorch:     627.9 ms
   TensorRT:    260.3 ms
   üìä Speedup:   2.41x
   üìâ R√©duction: 58.5%

D√âBIT:
   PyTorch:     864.4 tokens/s
   TensorRT:    2723.2 tokens/s
   üìä Speedup:   3.15x
   üìà Gain:      215.0%

M√âMOIRE GPU:
   PyTorch:     2.60 GB
   TensorRT:    2.61 GB
   üìä Ratio:     1.00x
   üìâ Variation: +0.1%
```

## ‚öôConfiguration Avanc√©e

### Personnaliser les Param√®tres de Benchmark

Modifiez les scripts dans `./scripts/` pour ajuster :

- **Nombre d'it√©rations** : `num_iterations=10` dans les scripts
- **Longueur de g√©n√©ration** : `max_new_tokens=200` (optimis√© pour TensorRT)
- **Param√®tres de sampling** : `temperature=0.7`, `top_p=0.9`
- **Taille de batch** : `max_batch_size=1` dans `build_engine.sh`
- **Prompts** : Utilisez des s√©quences longues pour maximiser les gains TensorRT

### Tester d'Autres Mod√®les

Dans `scripts/build_engine.sh`, changez :

```bash
MODEL_NAME="microsoft/DialoGPT-medium"  # Exemple
# ou
MODEL_NAME="facebook/opt-1.3b"
```

### Optimisations TensorRT-LLM Appliqu√©es

Ce projet utilise les optimisations suivantes pour maximiser les performances :

#### Optimisations Automatiques
- **torch.compile** : Compilation des graphes PyTorch pour de meilleures performances
- **FP16 precision** : Calculs en demi-pr√©cision pour r√©duire la latence  
- **KV-cache optimization** : Gestion optimis√©e du cache des cl√©s/valeurs
- **Kernel fusion** : Fusion des op√©rations GPU pour r√©duire les appels
- **Memory pooling** : Gestion optimis√©e de la m√©moire GPU

#### Configuration Optimis√©e
```python
# Dans les scripts de benchmark
model.eval()                    # Mode √©valuation
model.half()                   # Pr√©cision FP16  
torch.compile(model, mode='max-autotune')  # Compilation optimis√©e
use_cache=True                 # Cache KV activ√©
num_beams=1                    # Single beam pour latence
```

#### Pour des Gains Maximaux
- Utilisez des **prompts longs** (100+ tokens)
- G√©n√©rez des **s√©quences longues** (200+ tokens)
- Activez **torch.compile** (automatique dans ce projet)
- Testez avec des **mod√®les plus grands** (7B, 13B)

## D√©pannage

### Erreurs Courantes

**"CUDA out of memory"**
```bash
# R√©duire la taille du mod√®le ou les param√®tres
# Dans build_engine.sh :
--max_batch_size 1
--max_input_len 256
--max_output_len 256
```

**"TensorRT engine not found"**
```bash
# Reconstruire le moteur
docker-compose --profile build-only up build-engine
```

**"Docker can't access GPU"**
```bash
# V√©rifier l'installation nvidia-docker
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### Logs et Debug

```bash
# Voir les logs d√©taill√©s
docker-compose --profile auto up benchmark-full --verbose

# Mode debug interactif
docker-compose --profile manual run --rm tensorrt-llm-benchmark bash
```

## R√©sultats de Benchmark Valid√©s

### RTX 4070 - TinyLlama 1.1B (S√©quences Longues)

| M√©trique | PyTorch | TensorRT-LLM | **Gain TensorRT** |
|----------|---------|--------------|-------------------|
| **Latence moyenne** | 627.9 ms | 260.3 ms | **üöÄ 2.41x plus rapide** |
| **D√©bit moyen** | 864.4 tok/s | 2723.2 tok/s | **üöÄ 3.15x plus rapide** |
| **M√©moire GPU** | 2.60 GB | 2.61 GB | ‚âà Identique |

### Conditions de Test
- **GPU** : NVIDIA RTX 4070 (12GB VRAM)
- **Mod√®le** : TinyLlama-1.1B-Chat-v1.0
- **S√©quences** : Prompts longs (~100 tokens) + g√©n√©ration 200 tokens
- **Pr√©cision** : FP16
- **Configuration** : 10 it√©rations par benchmark

### Pourquoi Ces Gains ?

Les **s√©quences longues maximisent l'impact TensorRT-LLM** gr√¢ce √† :
- **Optimisations KV-cache** : Plus efficaces sur longues s√©quences
- **Fusion des kernels GPU** : R√©duction des appels GPU
- **Gestion m√©moire optimis√©e** : Moins de transferts CPU-GPU
- **torch.compile** : Compilation optimis√©e des graphes

*Les gains augmentent avec la longueur des s√©quences et la taille du mod√®le.*

## Personnalisation

### Ajouter d'Autres Mod√®les

1. Cr√©er un nouveau script `scripts/build_engine_<model>.sh`
2. Adapter les chemins et param√®tres du mod√®le
3. Ajouter un service dans `docker-compose.yml`

### M√©triques Suppl√©mentaires

Modifiez `scripts/compare.py` pour ajouter :
- Analyse de la qualit√© de g√©n√©ration (BLEU, perplexit√©)
- Profiling d√©taill√© du GPU
- Comparaison avec d'autres backends (ONNX, OpenVINO)

## Ressources

- [Documentation TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/)
- [Guide d'optimisation NVIDIA](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)
- [Mod√®les support√©s](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples)

## Contribution

Les contributions sont bienvenues ! Ouvrez une issue ou proposez une pull request pour :
- Support de nouveaux mod√®les
- Optimisations suppl√©mentaires  
- Am√©lioration des m√©triques
- Documentation

## Licence

MIT License - Voir [LICENSE](LICENSE) pour plus de d√©tails.

---

## D√©marrage Rapide

1. **Authentification NVIDIA** (obligatoire) :
   ```powershell
   .\setup_nvidia_auth.ps1
   ```

2. **Benchmark complet automatique** :
   ```bash
   docker-compose --profile auto up benchmark-full
   ```

3. **Consulter les r√©sultats** :
   ```bash
   # Voir le rapport
   cat data/results/benchmark_comparison_report.json
   
   # Ouvrir le graphique  
   start data/results/benchmark_comparison.png  # Windows
   open data/results/benchmark_comparison.png   # macOS
   ```

**R√©sultats attendus sur RTX 4070** : **2.4x latence** et **3.2x d√©bit** ! üöÄ
