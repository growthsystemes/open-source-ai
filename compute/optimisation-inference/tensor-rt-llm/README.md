#  Benchmark TensorRT-LLM sur TinyLlama avec Docker

Mesure le gain de performances qu'apporte [NVIDIA TensorRTâ€‘LLM](https://github.com/NVIDIA/TensorRT-LLM) avec un petit modÃ¨le openâ€‘source Hugging Face (TinyLlama 1.1B Chat) par rapport Ã  l'infÃ©rence PyTorch "pure". Tout le projet tourne dans un conteneur Docker prÃªt Ã  l'emploi : aucune compilation locale n'est nÃ©cessaire.

## ğŸ¯ Objectifs

- âœ… Reproduire facilement un flux complet : tÃ©lÃ©charger le modÃ¨le â†’ convertir/optimiser â†’ gÃ©nÃ©rer un moteur TensorRT â†’ benchmarker
- ğŸ“Š Obtenir des mÃ©triques latency & throughput Â« outâ€‘ofâ€‘theâ€‘box Â»
- ğŸ”§ Servir de point de dÃ©part pour tester d'autres LLM, quantisations ou GPUs

## ğŸ“ Arborescence

```
.
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile              # Image basÃ©e sur nvcr.io/nvidia/tensorrtâ€‘llm
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_engine.sh         # Construction du moteur TensorRT
â”‚   â”œâ”€â”€ bench_pytorch.sh        # Benchmark rÃ©fÃ©rence (PyTorch)
â”‚   â”œâ”€â”€ bench_trt.sh           # Benchmark optimisÃ© (TensorRT-LLM)
â”‚   â””â”€â”€ compare.py             # Analyse et comparaison des rÃ©sultats
â”œâ”€â”€ docker-compose.yml         # Services Docker pour faciliter l'utilisation
â”œâ”€â”€ data/                      # DonnÃ©es persistantes (crÃ©Ã© automatiquement)
â”‚   â”œâ”€â”€ models/               # ModÃ¨les tÃ©lÃ©chargÃ©s
â”‚   â”œâ”€â”€ engines/              # Moteurs TensorRT compilÃ©s
â”‚   â”œâ”€â”€ checkpoints/          # Checkpoints intermÃ©diaires
â”‚   â””â”€â”€ results/              # RÃ©sultats de benchmark
â””â”€â”€ README.md
```

## ğŸ› ï¸ PrÃ©requis

- **Docker** avec support GPU (`nvidia-docker` ou Docker Desktop avec GPU)
- **GPU NVIDIA** compatible CUDA (RTX 20xx/30xx/40xx recommandÃ©)
- **8+ GB de VRAM** (minimum pour TinyLlama 1.1B)
- **Drivers NVIDIA** rÃ©cents (525+)
- **ğŸ” Compte NVIDIA NGC** (gratuit) pour accÃ©der aux images TensorRT-LLM

### ğŸš¨ Ã‰TAPE OBLIGATOIRE : Authentification NVIDIA

**Avant de commencer**, vous devez vous authentifier sur le registry NVIDIA :

#### Option 1 : Script Automatique (RecommandÃ©)

**Windows :**
```powershell
.\setup_nvidia_auth.ps1
```

**Linux/macOS :**
```bash
chmod +x setup_nvidia_auth.sh
./setup_nvidia_auth.sh
```

#### Option 2 : Manuel

1. **CrÃ©er un compte NGC** sur [ngc.nvidia.com](https://ngc.nvidia.com) (gratuit)
2. **GÃ©nÃ©rer une clÃ© API** : Profile â†’ Setup â†’ Generate API Key
3. **Se connecter** :
   ```bash
   docker login nvcr.io
   # Username: $oauthtoken
   # Password: [votre_clÃ©_API_qui_commence_par_nvapi-]
   ```

ğŸ“‹ **Guide dÃ©taillÃ©** : Voir [NVIDIA_SETUP.md](NVIDIA_SETUP.md)

### VÃ©rification GPU

```bash
# VÃ©rifier que Docker peut accÃ©der au GPU
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu20.04 nvidia-smi
```

## ğŸš€ Utilisation Rapide

### ğŸ¯ Commande RecommandÃ©e (Une Ligne)

AprÃ¨s avoir configurÃ© l'authentification NVIDIA, lancez le benchmark complet :

```bash
docker-compose --profile auto up benchmark-full
```

**â±ï¸ DurÃ©e estimÃ©e** : 15-25 minutes selon votre GPU  
**ğŸ“ RÃ©sultats** : GÃ©nÃ©rÃ©s automatiquement dans `./data/results/`

### ğŸ“‹ Pipeline Complet ExÃ©cutÃ©

Cette commande exÃ©cute automatiquement :
1. ğŸ”§ **Construction du moteur TensorRT** (`build_engine.sh`)
2. ğŸ“Š **Benchmark PyTorch** (rÃ©fÃ©rence baseline)
3. âš¡ **Benchmark TensorRT-LLM** (version optimisÃ©e)
4. ğŸ“ˆ **Analyse comparative** (graphiques + rapport JSON)

### ğŸ”§ Ã‰tapes Manuelles (ContrÃ´le AvancÃ©)

Pour plus de contrÃ´le sur chaque Ã©tape :

```bash
# 1. Construction du moteur TensorRT-LLM
docker-compose --profile build-only up build-engine

# 2. Benchmark PyTorch (rÃ©fÃ©rence baseline)  
docker-compose --profile pytorch-only up benchmark-pytorch

# 3. Benchmark TensorRT-LLM (version optimisÃ©e)
docker-compose --profile tensorrt-only up benchmark-tensorrt

# 4. Comparaison et analyse des rÃ©sultats
docker-compose --profile compare-only up compare-results
```

### ğŸ” Mode Interactif (Debug/Exploration)

Pour explorer le systÃ¨me ou dÃ©boguer :

```bash
# DÃ©marrage du container en mode interactif
docker-compose --profile manual run --rm tensorrt-llm-benchmark bash

# Dans le container, vous pouvez exÃ©cuter :
bash scripts/build_engine.sh
bash scripts/bench_pytorch.sh  
bash scripts/bench_trt.sh
python3 scripts/compare.py

# Ou explorer les modÃ¨les/engines :
ls -la data/models/
ls -la data/engines/
cat data/results/pytorch_benchmark.json
```

## ğŸ“Š RÃ©sultats

AprÃ¨s exÃ©cution, vous trouverez dans `./data/results/` :

### Fichiers GÃ©nÃ©rÃ©s

- `pytorch_benchmark.json` - MÃ©triques dÃ©taillÃ©es PyTorch
- `tensorrt_benchmark.json` - MÃ©triques dÃ©taillÃ©es TensorRT-LLM  
- `benchmark_comparison_report.json` - Rapport de comparaison complet
- `benchmark_comparison.png` - Graphiques de comparaison

### MÃ©triques AnalysÃ©es

- **â±ï¸ Latence** : Temps de gÃ©nÃ©ration par sÃ©quence (ms)
- **âš¡ DÃ©bit** : Tokens gÃ©nÃ©rÃ©s par seconde
- **ğŸ’¾ MÃ©moire GPU** : Utilisation VRAM (GB)
- **ğŸ“ˆ Speedup** : Facteur d'amÃ©lioration TensorRT vs PyTorch

### Exemple de Sortie RÃ©elle (RTX 4070)

```
ğŸš€ RAPPORT DE PERFORMANCE - TensorRT-LLM vs PyTorch
============================================================

ğŸ“ˆ LATENCE:
   PyTorch:     627.9 ms
   TensorRT:    260.3 ms
   ğŸ“Š Speedup:   2.41x
   ğŸ“‰ RÃ©duction: 58.5%

âš¡ DÃ‰BIT:
   PyTorch:     864.4 tokens/s
   TensorRT:    2723.2 tokens/s
   ğŸ“Š Speedup:   3.15x
   ğŸ“ˆ Gain:      215.0%

ğŸ’¾ MÃ‰MOIRE GPU:
   PyTorch:     2.60 GB
   TensorRT:    2.61 GB
   ğŸ“Š Ratio:     1.00x
   ğŸ“‰ Variation: +0.1%
```

## âš™ï¸ Configuration AvancÃ©e

### Personnaliser les ParamÃ¨tres de Benchmark

Modifiez les scripts dans `./scripts/` pour ajuster :

- **Nombre d'itÃ©rations** : `num_iterations=10` dans les scripts
- **Longueur de gÃ©nÃ©ration** : `max_new_tokens=200` (optimisÃ© pour TensorRT)
- **ParamÃ¨tres de sampling** : `temperature=0.7`, `top_p=0.9`
- **Taille de batch** : `max_batch_size=1` dans `build_engine.sh`
- **Prompts** : Utilisez des sÃ©quences longues pour maximiser les gains TensorRT

### Tester d'Autres ModÃ¨les

Dans `scripts/build_engine.sh`, changez :

```bash
MODEL_NAME="microsoft/DialoGPT-medium"  # Exemple
# ou
MODEL_NAME="facebook/opt-1.3b"
```

### Optimisations TensorRT-LLM AppliquÃ©es

Ce projet utilise les optimisations suivantes pour maximiser les performances :

#### ğŸš€ Optimisations Automatiques
- **torch.compile** : Compilation des graphes PyTorch pour de meilleures performances
- **FP16 precision** : Calculs en demi-prÃ©cision pour rÃ©duire la latence  
- **KV-cache optimization** : Gestion optimisÃ©e du cache des clÃ©s/valeurs
- **Kernel fusion** : Fusion des opÃ©rations GPU pour rÃ©duire les appels
- **Memory pooling** : Gestion optimisÃ©e de la mÃ©moire GPU

#### âš™ï¸ Configuration OptimisÃ©e
```python
# Dans les scripts de benchmark
model.eval()                    # Mode Ã©valuation
model.half()                   # PrÃ©cision FP16  
torch.compile(model, mode='max-autotune')  # Compilation optimisÃ©e
use_cache=True                 # Cache KV activÃ©
num_beams=1                    # Single beam pour latence
```

#### ğŸ“ˆ Pour des Gains Maximaux
- Utilisez des **prompts longs** (100+ tokens)
- GÃ©nÃ©rez des **sÃ©quences longues** (200+ tokens)
- Activez **torch.compile** (automatique dans ce projet)
- Testez avec des **modÃ¨les plus grands** (7B, 13B)

## ğŸ› DÃ©pannage

### Erreurs Courantes

**"CUDA out of memory"**
```bash
# RÃ©duire la taille du modÃ¨le ou les paramÃ¨tres
# Dans build_engine.sh :
--max_batch_size 1
--max_input_len 256
--max_output_len 256
```

**"TensorRT engine not found"**
```bash
# Reconstruire le moteur
docker-compose --profile build-only up build-engine
```

**"Docker can't access GPU"**
```bash
# VÃ©rifier l'installation nvidia-docker
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### Logs et Debug

```bash
# Voir les logs dÃ©taillÃ©s
docker-compose --profile auto up benchmark-full --verbose

# Mode debug interactif
docker-compose --profile manual run --rm tensorrt-llm-benchmark bash
```

## ğŸ“ˆ RÃ©sultats de Benchmark ValidÃ©s

### RTX 4070 - TinyLlama 1.1B (SÃ©quences Longues)

| MÃ©trique | PyTorch | TensorRT-LLM | **Gain TensorRT** |
|----------|---------|--------------|-------------------|
| **Latence moyenne** | 627.9 ms | 260.3 ms | **ğŸš€ 2.41x plus rapide** |
| **DÃ©bit moyen** | 864.4 tok/s | 2723.2 tok/s | **ğŸš€ 3.15x plus rapide** |
| **MÃ©moire GPU** | 2.60 GB | 2.61 GB | â‰ˆ Identique |

### Conditions de Test
- **GPU** : NVIDIA RTX 4070 (12GB VRAM)
- **ModÃ¨le** : TinyLlama-1.1B-Chat-v1.0
- **SÃ©quences** : Prompts longs (~100 tokens) + gÃ©nÃ©ration 200 tokens
- **PrÃ©cision** : FP16
- **Configuration** : 10 itÃ©rations par benchmark

### Pourquoi Ces Gains ?

Les **sÃ©quences longues maximisent l'impact TensorRT-LLM** grÃ¢ce Ã  :
- âœ… **Optimisations KV-cache** : Plus efficaces sur longues sÃ©quences
- âœ… **Fusion des kernels GPU** : RÃ©duction des appels GPU
- âœ… **Gestion mÃ©moire optimisÃ©e** : Moins de transferts CPU-GPU
- âœ… **torch.compile** : Compilation optimisÃ©e des graphes

*Les gains augmentent avec la longueur des sÃ©quences et la taille du modÃ¨le.*

## ğŸ”§ Personnalisation

### Ajouter d'Autres ModÃ¨les

1. CrÃ©er un nouveau script `scripts/build_engine_<model>.sh`
2. Adapter les chemins et paramÃ¨tres du modÃ¨le
3. Ajouter un service dans `docker-compose.yml`

### MÃ©triques SupplÃ©mentaires

Modifiez `scripts/compare.py` pour ajouter :
- Analyse de la qualitÃ© de gÃ©nÃ©ration (BLEU, perplexitÃ©)
- Profiling dÃ©taillÃ© du GPU
- Comparaison avec d'autres backends (ONNX, OpenVINO)

## ğŸ“š Ressources

- [Documentation TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/)
- [Guide d'optimisation NVIDIA](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)
- [ModÃ¨les supportÃ©s](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples)

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Ouvrez une issue ou proposez une pull request pour :
- Support de nouveaux modÃ¨les
- Optimisations supplÃ©mentaires  
- AmÃ©lioration des mÃ©triques
- Documentation

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸš€ DÃ©marrage Rapide

1. **Authentification NVIDIA** (obligatoire) :
   ```powershell
   .\setup_nvidia_auth.ps1
   ```

2. **Benchmark complet automatique** :
   ```bash
   docker-compose --profile auto up benchmark-full
   ```

3. **Consulter les rÃ©sultats** :
   ```bash
   # Voir le rapport
   cat data/results/benchmark_comparison_report.json
   
   # Ouvrir le graphique  
   start data/results/benchmark_comparison.png  # Windows
   open data/results/benchmark_comparison.png   # macOS
   ```

**ğŸ¯ RÃ©sultats attendus sur RTX 4070** : **2.4x latence** et **3.2x dÃ©bit** ! ğŸš€