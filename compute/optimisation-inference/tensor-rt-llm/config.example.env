# Configuration pour TensorRT-LLM Benchmark
# Copiez ce fichier vers .env et ajustez les valeurs

# Modèle à benchmarker
MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
# Alternatives populaires:
# MODEL_NAME=microsoft/DialoGPT-medium
# MODEL_NAME=facebook/opt-1.3b
# MODEL_NAME=EleutherAI/gpt-neo-1.3B

# Paramètres de benchmark
BENCHMARK_ITERATIONS=10
MAX_NEW_TOKENS=100
MAX_INPUT_LENGTH=512
MAX_OUTPUT_LENGTH=512
MAX_BATCH_SIZE=1

# Paramètres de génération
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50

# Paramètres TensorRT
USE_FP16=true
USE_GEMMN_PLUGIN=true
USE_GPT_ATTENTION_PLUGIN=true
USE_FUSED_MLP=false

# GPU et mémoire
CUDA_VISIBLE_DEVICES=0
GPU_MEMORY_FRACTION=0.9

# Chemins (relatifs au container)
MODELS_DIR=/workspace/models
ENGINES_DIR=/workspace/engines
CHECKPOINTS_DIR=/workspace/checkpoints
RESULTS_DIR=/workspace/results

# Options d'affichage
VERBOSE=true
SAVE_PLOTS=true
SHOW_PROGRESS=true
