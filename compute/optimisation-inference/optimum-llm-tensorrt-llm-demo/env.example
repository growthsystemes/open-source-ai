# Configuration d'environnement pour inference-optim-llm
# Copiez ce fichier vers .env et adaptez les valeurs

# Configuration du mod√®le
MODEL_ID=meta-llama/Llama-2-7b-chat-hf
# MODEL_ID=gpt2  # Pour tests rapides

# Configuration TensorRT-LLM
TRT_PRECISION=fp16
QUANT_MODE=

# Configuration GPU
CUDA_VISIBLE_DEVICES=0

# Configuration de benchmark
BATCH_SIZE=1
MAX_NEW_TOKENS=64

# Chemins et cache
HF_HOME=/workspace/.cache/huggingface