# üê≥ Guide Docker - Inference Optim LLM

Docker est la **solution recommand√©e** pour √©viter tous les probl√®mes d'environnement !

## üöÄ D√©marrage Rapide (5 minutes)

### 1. Pr√©paration
```bash
# Copiez la configuration
cp env.docker.example .env

# Rendez le script ex√©cutable (Linux/Mac)
chmod +x docker-run.sh
```

### 2. Test Baseline (Sans GPU)
```bash
# Linux/Mac
./docker-run.sh baseline run

# Windows PowerShell
docker-compose --profile baseline up --build
```

### 3. Test Complet avec GPU
```bash
# Linux/Mac
./docker-run.sh bench run

# Windows PowerShell
docker-compose --profile bench up --build
```

## üìã Commandes Disponibles

### Via Script (Linux/Mac)
```bash
# Construction des images
./docker-run.sh baseline build
./docker-run.sh trtllm build

# Ex√©cution
./docker-run.sh baseline run
./docker-run.sh trtllm run
./docker-run.sh bench run

# Mode d√©veloppement
./docker-run.sh dev shell

# Nettoyage
./docker-run.sh clean
```

### Via Docker Compose Direct
```bash
# Baseline seulement
docker-compose --profile baseline up --build

# TensorRT-LLM seulement  
docker-compose --profile trtllm up --build

# Benchmark complet
docker-compose --profile bench up --build

# Mode d√©veloppement
docker-compose --profile dev up --build

# Nettoyage
docker-compose down --volumes
```

## üîß Configuration

### Variables d'Environnement (.env)
```bash
MODEL_ID=gpt2                    # Mod√®le √† utiliser
BATCH_SIZE=1                     # Taille de batch
MAX_NEW_TOKENS=32               # Tokens √† g√©n√©rer
CUDA_VISIBLE_DEVICES=0          # GPU √† utiliser
TRT_PRECISION=fp16              # Pr√©cision TensorRT
```

### Mod√®les Recommand√©s
```bash
# Test rapide (CPU OK)
MODEL_ID=gpt2

# Production l√©g√®re
MODEL_ID=microsoft/DialoGPT-small

# Production compl√®te (GPU recommand√©)
MODEL_ID=meta-llama/Llama-2-7b-chat-hf
```

## üìä R√©sultats

Les r√©sultats sont sauvegard√©s dans `./reports/` :
- `baseline.jsonl` - M√©triques baseline
- `trtllm.jsonl` - M√©triques TensorRT-LLM
- `benchmark_results.md` - Rapport comparatif

## üõ†Ô∏è D√©veloppement

### Mode Interactif
```bash
# Ouvrir un shell dans le conteneur
./docker-run.sh dev shell

# Ou avec docker-compose
docker-compose --profile dev run --rm dev bash
```

### Tests dans le Conteneur
```bash
# Une fois dans le conteneur
python -m inference_optim_llm.cli --help
python -m inference_optim_llm.cli run baseline --max-new-tokens 10
python scripts/validate_setup.py
```

## üêõ R√©solution de Probl√®mes

### Erreur GPU
```bash
# V√©rifiez NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi

# Utilisez CPU seulement
export CUDA_VISIBLE_DEVICES=""
./docker-run.sh baseline run
```

### Probl√®me de Build
```bash
# Nettoyage complet
./docker-run.sh clean
docker system prune -a

# Rebuild from scratch
./docker-run.sh baseline build
```

### Probl√®me Permissions (Linux)
```bash
# Ajustez les permissions
sudo chown -R $USER:$USER ./reports
sudo chown -R $USER:$USER ./engines
```

## üìà Benchmark Complet

### Lancement
```bash
# Benchmark avec GPU
CUDA_VISIBLE_DEVICES=0 ./docker-run.sh bench run

# Benchmark CPU seulement
CUDA_VISIBLE_DEVICES="" ./docker-run.sh bench run
```

### Analyse des R√©sultats
```bash
# Voir le rapport
cat reports/benchmark_results.md

# Voir les m√©triques raw
cat reports/baseline.jsonl
cat reports/trtllm.jsonl
```

## üéØ Cas d'Usage

### 1. Test Rapide Baseline
```bash
MODEL_ID=gpt2 MAX_NEW_TOKENS=16 ./docker-run.sh baseline run
```

### 2. Benchmark Production
```bash
MODEL_ID=meta-llama/Llama-2-7b-chat-hf ./docker-run.sh bench run
```

### 3. D√©veloppement Interactif
```bash
./docker-run.sh dev shell
# Dans le conteneur: modifier le code et tester
```

## ‚úÖ Avantages Docker

- ‚úÖ **Pas de probl√®mes Python/PyTorch**
- ‚úÖ **Environnement reproductible**
- ‚úÖ **Installation automatique des d√©pendances**
- ‚úÖ **Isolation compl√®te**
- ‚úÖ **Support GPU automatique**
- ‚úÖ **Cache des mod√®les persistant**

**üéâ Docker = Solution universelle qui marche partout !**