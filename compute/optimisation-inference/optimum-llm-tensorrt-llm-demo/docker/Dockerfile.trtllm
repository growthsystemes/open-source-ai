# Dockerfile pour environnement TensorRT-LLM
FROM nvcr.io/nvidia/tensorrt:24.02-py3

# Metadonnees
LABEL maintainer="inference-optim-llm"
LABEL description="TensorRT-LLM optimized inference environment"

# Arguments de build
ARG MODEL_ID=gpt2
ARG BATCH_SIZE=1
ARG MAX_NEW_TOKENS=64
ARG PRECISION=fp16

# Variables d'environnement
ENV MODEL_ID=${MODEL_ID}
ENV BATCH_SIZE=${BATCH_SIZE}
ENV MAX_NEW_TOKENS=${MAX_NEW_TOKENS}
ENV TRT_PRECISION=${PRECISION}
ENV PYTHONPATH=/workspace
ENV HF_HOME=/workspace/.cache/huggingface
ENV TRANSFORMERS_CACHE=/workspace/.cache/huggingface/transformers
ENV HF_HUB_CACHE=/workspace/.cache/huggingface/hub

# Installation des dependances systeme
RUN apt-get update && apt-get install -y \
    git \
    git-lfs \
    wget \
    curl \
    build-essential \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

# Repertoire de travail
WORKDIR /workspace

# Installation TensorRT-LLM
# Note: Adapte selon la version disponible
RUN pip install --extra-index-url https://pypi.nvidia.com tensorrt-llm>=0.7.0 || \
    echo "WARNING: TensorRT-LLM not installed - use appropriate NGC image"

# Installation des dependances Python
COPY requirements-trtllm.txt .
RUN pip install --no-cache-dir -r requirements-trtllm.txt

# Copie du code source
COPY inference_optim_llm/ ./inference_optim_llm/
COPY scripts/ ./scripts/
COPY data/ ./data/
COPY docker/test_tensorrt.py ./test_tensorrt.py
COPY pyproject.toml .
COPY README.md .

# Installation du package local
RUN pip install -e .

# Creation des repertoires necessaires
RUN mkdir -p /workspace/reports /workspace/models /workspace/engines /workspace/.cache

# Test de l'installation
RUN python -c "from inference_optim_llm.core.metrics import MetricsCollector; print('Package OK')" && \
    python -c "import torch; print('PyTorch OK')" && \
    python -c "import transformers; print('Transformers OK')" && \
    python test_tensorrt.py

# Telechargement du modele et build de l'engine
RUN echo "Preparing model ${MODEL_ID}..." && \
    python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; print('Downloading model ${MODEL_ID}...'); tokenizer = AutoTokenizer.from_pretrained('${MODEL_ID}'); model = AutoModelForCausalLM.from_pretrained('${MODEL_ID}'); print('Model downloaded')" && \
    echo "Building TensorRT-LLM engine..." && \
    (python -m inference_optim_llm.cli build --model-id ${MODEL_ID} --precision ${PRECISION} || echo "Engine build failed - will be done at runtime")

# Utilisateur non-root pour la securite
RUN useradd -m -u 1000 llmuser && chown -R llmuser:llmuser /workspace
USER llmuser

# Point d'entree
ENTRYPOINT ["python", "-m", "inference_optim_llm.cli"]
CMD ["run", "trtllm", "--help"]