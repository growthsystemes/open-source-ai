import { openai } from '@ai-sdk/openai';
import {
  convertToModelMessages,
  createUIMessageStream,
  createUIMessageStreamResponse,
  generateText,
  streamText,
  type ModelMessage,
  type UIMessage,
  type UIMessageStreamWriter,
} from 'ai';
import { SYSTEME_GUARDRAIL } from './guardrail-prompt.ts';
import { CONFIG } from '../config/environnement.ts';

/**
 * Interface pour les m√©triques de s√©curit√©
 */
interface MetriqueSecurite {
  timestamp: number;
  estSure: boolean;
  raisonBlocage?: string;
  tempsVerification: number;
}

/**
 * Cache simple pour √©viter les v√©rifications r√©p√©titives
 */
const cacheVerifications = new Map<string, MetriqueSecurite>();

/**
 * V√©rifie si une requ√™te est s√©curis√©e en utilisant notre syst√®me de guardrails
 * @param messagesModele - Messages de la conversation √† analyser
 * @returns Promise<boolean> - true si la requ√™te est s√ªre, false sinon
 */
const verifierSecuriteRequete = async (
  messagesModele: ModelMessage[],
): Promise<{ estSure: boolean; tempsVerification: number; raisonBlocage?: string }> => {
  const debut = Date.now();
  
  // G√©n√©rer une cl√© de cache bas√©e sur le contenu des messages
  const cleCache = messagesModele
    .map(msg => `${msg.role}:${msg.content}`)
    .join('|');
  
  // V√©rifier le cache (expire apr√®s 5 minutes)
  const cacheExistant = cacheVerifications.get(cleCache);
  if (cacheExistant && Date.now() - cacheExistant.timestamp < 5 * 60 * 1000) {
    return {
      estSure: cacheExistant.estSure,
      tempsVerification: Date.now() - debut,
      raisonBlocage: cacheExistant.raisonBlocage
    };
  }

  try {
    // Analyse de s√©curit√© avec le mod√®le l√©ger
    const resultatGuardrail = await generateText({
      model: openai(CONFIG.GUARDRAIL_MODEL),
      system: SYSTEME_GUARDRAIL,
      messages: messagesModele,
      maxTokens: 1, // Optimisation : un seul token de r√©ponse (0 ou 1)
      temperature: 0, // Temp√©rature z√©ro pour r√©ponse d√©terministe
    });

    const texte = resultatGuardrail.text.trim();
    const estSure = texte === '1';
    const tempsVerification = Date.now() - debut;
    
    // Mise en cache du r√©sultat
    const metrique: MetriqueSecurite = {
      timestamp: Date.now(),
      estSure,
      raisonBlocage: estSure ? undefined : 'Contenu d√©tect√© comme potentiellement dangereux',
      tempsVerification
    };
    cacheVerifications.set(cleCache, metrique);

    return {
      estSure,
      tempsVerification,
      raisonBlocage: metrique.raisonBlocage
    };
    
  } catch (erreur) {
    // En cas d'erreur, on privil√©gie la s√©curit√© en bloquant
    console.error('Erreur lors de la v√©rification de s√©curit√©:', erreur);
    return {
      estSure: false,
      tempsVerification: Date.now() - debut,
      raisonBlocage: 'Erreur technique lors de la v√©rification de s√©curit√©'
    };
  }
};

/**
 * Envoie un message texte vers le client via le stream
 * @param writer - Writer du stream UI
 * @param message - Message √† envoyer
 */
const diffuserTexteVersWriter = (
  writer: UIMessageStreamWriter,
  message: string,
) => {
  const id = crypto.randomUUID();
  
  writer.write({
    type: 'text-start',
    id,
  });

  writer.write({
    type: 'text-delta',
    id,
    delta: message,
  });

  writer.write({
    type: 'text-end',
    id,
  });
};

/**
 * Continue le streaming avec le mod√®le principal apr√®s validation s√©curitaire
 * @param writer - Writer du stream UI
 * @param messagesModele - Messages valid√©s √† traiter
 */
const continuerStreaming = (
  writer: UIMessageStreamWriter,
  messagesModele: ModelMessage[],
) => {
  const resultatStreamTexte = streamText({
    model: openai(CONFIG.MAIN_MODEL),
    messages: messagesModele,
    // Param√®tres optimis√©s pour une meilleure qualit√© de r√©ponse
    temperature: 0.7,
    maxTokens: 2000,
  });

  writer.merge(resultatStreamTexte.toUIMessageStream());
};

/**
 * Point d'entr√©e principal de l'API chat
 * G√®re le pipeline complet : s√©curit√© ‚Üí traitement ‚Üí r√©ponse
 */
export const POST = async (
  req: Request,
): Promise<Response> => {
  try {
    // Conversion des messages UI vers le format du mod√®le
    const messagesModele = convertToModelMessages(
      (await req.json()).messages,
    );

    // Cr√©ation du stream de r√©ponse
    const stream = createUIMessageStream({
      execute: async ({ writer }) => {
        // √âtape 1: V√©rification de s√©curit√© avec m√©triques
        console.log('üîç Analyse de s√©curit√© en cours...');
        const {
          estSure,
          tempsVerification,
          raisonBlocage
        } = await verifierSecuriteRequete(messagesModele);

        console.log(`‚è±Ô∏è  V√©rification termin√©e en ${tempsVerification}ms - R√©sultat: ${estSure ? '‚úÖ S√ªr' : '‚ùå Bloqu√©'}`);

        // √âtape 2: Gestion des requ√™tes non-s√ªres
        if (!estSure) {
          const messageBlocage = `üö´ **Demande bloqu√©e par les guardrails de s√©curit√©**

${raisonBlocage}

Je ne peux pas traiter cette demande car elle pourrait violer nos directives de s√©curit√©. 

üí° **Suggestions alternatives :**
- Reformulez votre question de mani√®re plus sp√©cifique
- Consultez notre documentation sur les usages acceptables`;

          diffuserTexteVersWriter(writer, messageBlocage);
          return;
        }

        // √âtape 3: Traitement normal de la requ√™te valid√©e
        console.log('‚úÖ Requ√™te valid√©e - G√©n√©ration de la r√©ponse...');
        continuerStreaming(writer, messagesModele);
      },
    });

    return createUIMessageStreamResponse({
      stream,
      headers: {
        'X-Content-Type-Options': 'nosniff',
        'X-Frame-Options': 'DENY',
        'X-XSS-Protection': '1; mode=block'
      }
    });
    
  } catch (erreur) {
    console.error('Erreur dans l\'API chat:', erreur);
    
    // R√©ponse d'erreur s√©curis√©e
    return new Response(
      JSON.stringify({
        error: 'Une erreur technique est survenue. Veuillez r√©essayer.'
      }),
      {
        status: 500,
        headers: {
          'Content-Type': 'application/json',
          'X-Content-Type-Options': 'nosniff'
        }
      }
    );
  }
};